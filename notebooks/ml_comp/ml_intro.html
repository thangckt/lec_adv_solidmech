
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction to machine learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/full_width.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/monkey.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Overview" href="ml_comp_intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/monkey.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computational Mechanics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../computational/comp_mech_intro.html">
   Introduction to Comptational Mechanics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../computational/initial_value_problem.html">
   Initial Value Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../computational/boundary_value_problem.html">
   Initial Value Problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ML in Comp Mech
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ml_comp_intro.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to machine learning
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/thangckt/note_comp_mech/edit/main/notebooks/ml_comp/ml_intro.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploring-parameter-spaces">
   Exploring parameter spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generating-data-from-simulations">
   Generating data from simulations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#normalizing-data">
   Normalizing data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-selection-and-design">
   Feature selection and design
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning">
   Deep learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-case-overview">
     Test case overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-feed-forward-neural-networks">
     Simple feed-forward neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-datasets">
     Preparing the datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generic-training-loop">
     Generic training loop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-with-batches-of-data">
     Training with batches of data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate-adjustment">
     Learning rate adjustment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions">
     Activation functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advanced-building-blocks">
     Advanced building blocks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dealing-with-uncertainty">
     Dealing with uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-prediction-errors">
     Visualizing prediction errors
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to machine learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploring-parameter-spaces">
   Exploring parameter spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generating-data-from-simulations">
   Generating data from simulations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#normalizing-data">
   Normalizing data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-selection-and-design">
   Feature selection and design
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning">
   Deep learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-case-overview">
     Test case overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-feed-forward-neural-networks">
     Simple feed-forward neural networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-datasets">
     Preparing the datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generic-training-loop">
     Generic training loop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-with-batches-of-data">
     Training with batches of data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-rate-adjustment">
     Learning rate adjustment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions">
     Activation functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#advanced-building-blocks">
     Advanced building blocks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dealing-with-uncertainty">
     Dealing with uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-prediction-errors">
     Visualizing prediction errors
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-machine-learning">
<h1>Introduction to machine learning<a class="headerlink" href="#introduction-to-machine-learning" title="Permalink to this headline">#</a></h1>
<p>In this notebook, we cover the following topics:</p>
<ul class="simple">
<li><p>Exploring parameters spaces</p></li>
<li><p>Generating data with simulations</p></li>
<li><p>Normalizing data</p></li>
<li><p>Feature selection and design</p></li>
<li><p>Deep learning</p>
<ul>
<li><p>Test case overview</p></li>
<li><p>Simple feed-forward neural networks</p></li>
<li><p>Preparing the datasets</p></li>
<li><p>Generic training loop</p></li>
<li><p>Training with batches of data</p></li>
<li><p>Learning rate adjustment</p></li>
<li><p>Activation functions</p></li>
<li><p>Advanced building blocks</p></li>
<li><p>Dealing with uncertainty</p></li>
<li><p>Visualizing prediction errors</p></li>
</ul>
</li>
</ul>
<p><strong>Note:</strong> if you receive an import error related to the <em>paraview</em> module when executing the cell below, update the version of flowTorch as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># enter the Python environment</span>
<span class="n">source</span> <span class="n">ml</span><span class="o">-</span><span class="n">cfd</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
<span class="c1"># remove the old version of flowTorch</span>
<span class="n">pip</span> <span class="n">uninstall</span> <span class="n">flowtorch</span>
<span class="c1"># install the latest version</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">FlowModelingControl</span><span class="o">/</span><span class="n">flowtorch</span><span class="o">.</span><span class="n">git</span><span class="nd">@aweiner</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">isdir</span><span class="p">,</span> <span class="n">join</span>
<span class="kn">from</span> <span class="nn">shutil</span> <span class="kn">import</span> <span class="n">copy</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">pt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">flowtorch.data</span> <span class="kn">import</span> <span class="n">FOAMDataloader</span>

<span class="c1"># make results reproducible</span>
<span class="n">pt</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">130</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">8</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="ne">----&gt; </span><span class="mi">8</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">pt</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;numpy&#39;
</pre></div>
</div>
</div>
</div>
<section id="exploring-parameter-spaces">
<h2>Exploring parameter spaces<a class="headerlink" href="#exploring-parameter-spaces" title="Permalink to this headline">#</a></h2>
<p>Suppose we want to conduct a CFD parameter study in which we vary the Reynolds number <span class="math notranslate nohighlight">\(Re\)</span> and record the drag coefficient <span class="math notranslate nohighlight">\(c_d\)</span>. Our aim would be to create an ML-based surrogate model based on the pairs <span class="math notranslate nohighlight">\(Re_i\)</span> and <span class="math notranslate nohighlight">\(c_{d,i}\)</span>. Let’s assume we know a range for <span class="math notranslate nohighlight">\(Re\)</span> that we are particularly interested in. An important question before starting the simulations is now how to pick suitable <span class="math notranslate nohighlight">\(Re\)</span> values in the specified range. In general, we want our sampling to be:</p>
<ol class="simple">
<li><p>unbiased - every point has the same chance of being selected</p></li>
<li><p>efficient - we want to run as few simulations as possible</p></li>
<li><p>homogeneous - we want the samples to be homogeneously distributed across the parameter space</p></li>
</ol>
<p>Let’s say we want to run <span class="math notranslate nohighlight">\(N_s\)</span> simulations. Intuitively, you may want to distribute the <span class="math notranslate nohighlight">\(N_s\)</span> points equally spaced across the Reynolds number range, and there are good arguments to do so. However, there are at least two problems with this approach:</p>
<ol class="simple">
<li><p>it does not scale to multiple dimensions, e.g., if we vary more than one parameter; if we want to treat every parameter equal, we should pick <span class="math notranslate nohighlight">\(N_s\)</span> points in every parameter range and perform simulation for all parameter permutations; for <span class="math notranslate nohighlight">\(d\)</span> parameters, this strategy would require running <span class="math notranslate nohighlight">\(N_s^d\)</span> simulations; the number of sample points increases exponentially</p></li>
<li><p>it is biased because not every point has the same chance of being sampled; the plot below show a somewhat artificial and extreme effect of bias; the true signal is periodic but the samples suggest that there is basically no correlation between input and output</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x_sample_1</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>
<span class="n">x_sample_2</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">3.375</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">130</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true function&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_sample_1</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_sample_1</span><span class="p">)</span> <span class="o">+</span>
            <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sample 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_sample_2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_sample_2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="o">*</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sample 2&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.02</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_3_0.png" src="../../_images/ml_intro_3_0.png" />
</div>
</div>
<p>To make the samples unbiased, we should select the samples randomly. However, selecting <span class="math notranslate nohighlight">\(N_s\)</span> points at random may give you some stomach ache, too, because large portions of the parameter space might remain unexplored. Random sampling is relatively inefficient.</p>
<p>A good compromise between the homogeneity of equally spaced sample points and efficient unbiased sampling is <em>latin hypercube sampling</em> (LHS). If the goal is to perform <span class="math notranslate nohighlight">\(N_s\)</span> simulations, each parameter range is equally divided into <span class="math notranslate nohighlight">\(N_s\)</span> sections. Then, one random sample is drawn in each section of each parameter. If there is more than one parameter, the samples drawn from the individual parameter ranges are shuffled. Combining (stacking) the shuffled individual samples yields the final samples in the full parameter space. Below, we implement a simple version of LHS in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lhs_sampling</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_min</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_max</span><span class="p">)</span>
    <span class="n">n_parameters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_min</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_parameters</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)):</span>
        <span class="n">bounds</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rand</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">bounds</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">samples</span>


<span class="n">N_s</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">lhs_sampling</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">N_s</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">3.375</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">130</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;samples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">N_s</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">N_s</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;parameter 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;parameter 2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;latin hypercube sampling&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_5_0.png" src="../../_images/ml_intro_5_0.png" />
</div>
</div>
</section>
<section id="generating-data-from-simulations">
<h2>Generating data from simulations<a class="headerlink" href="#generating-data-from-simulations" title="Permalink to this headline">#</a></h2>
<p>The previous section covered how to sample a parameter space efficiently and unbiased. This section outlines some tools and strategies to run automated CFD parameter studies. Every parameter study should start with a tested and validated base simulation. Typical criteria for a good base simulation are similar to the ones of any good CFD simulation:</p>
<ul class="simple">
<li><p>the setup correctly reflects the mathematical problem</p></li>
<li><p>a mesh dependency study was performed, and the results show sufficiently low mesh dependency</p></li>
<li><p>key performance quantities were validated against high-fidelity reference data</p></li>
<li><p>the setup is optimized; mesh-independent results are obtained as quickly as possible</p></li>
</ul>
<p>The easiest way to perform a parameter study is to create a copy of the base simulation, modify the parameters in question, and run the new setup. However, the change of one parameter often requires the adjustment of related settings, too. For example, increasing the inlet velocity or the pressure gradient driving the flow leads to an increased Courant number if the mesh remains unchanged. Since the increased Courant number might cause stability issues, the numerical time step should be lowered. Tools like OpenFOAM support time step adjustment based on the maximum Courant number, which simplifies parameter changes to some extend. If the time step needs adjustment, the characteristic time scale of the problem has probably changed, too. For example, the vortex shedding frequency displayed by the flow past a cylinder increases proportionally to the Reynolds number in the range <span class="math notranslate nohighlight">\(80 &lt; Re &lt; 1000\)</span>. If our aim is to run the simulation until a quasi-steady vortex shedding is reached, the required physical simulation time decreases as the Reynolds number increases. Running all simulation up to the same physical end time would be a waste of resources.</p>
<p>On the other hand side, changing too many parameters at once might make the setup unnecessarily complex and could introduce unforeseeable side effects. Mesh generation, for example, is often an intricate and time consuming process, such that we may want to run all simulations employing the same mesh. However, how do we ensure that the results remain mesh independent despite an increase, say, of the Reynolds number? A simple strategy for this scenario would be to pick the most demanding case in terms of mesh resolution and to build the mesh based on this configuration. Of course, the computational efficiency for less demanding cases decreases, but building optimized meshes for all conditions might be untractable.</p>
<p>Once the base case is ready, we can write a script that copies, modifies, and runs new setups. Shell or Python scripts are good choices to automate the process. Shell programming is a good option for simple parameter variations and to automate workflows in general. The <em>Allrun</em> scripts provided in the <em>test_cases</em> are simple recipes to perform a list processing steps required to perform the simulation. Python scripts are more powerful but require a Python interpreter and possibly additional library dependencies. Python also allows executing shell commands via the standard library functions <a class="reference external" href="https://docs.python.org/3/library/os.html#os.system">os.system()</a> or <a class="reference external" href="https://docs.python.org/3/library/subprocess.html#subprocess.Popen">subprocess.Popen()</a>. The <em>system</em> function is rather limited but sufficient for running simple and short shell commands. For example, the lines</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmd</span> <span class="o">=</span> <span class="s2">&quot;sed -i &#39;s/old_text/new_text/&#39; exampleDict&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
</pre></div>
</div>
<p>would search the file <em>exampleDict</em> for the string <em>old_text</em> and replace the text with <em>new_text</em>. On the other hand side, <em>Popen</em> offers greater flexibility in terms of process control. The same command as above executed with <em>subprocess</em> would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sed&quot;</span><span class="p">,</span> <span class="s2">&quot;-i&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;s/old_text/new_text/&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;exampleDict&quot;</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
<span class="c1"># print exit code to check if execution was successful</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">poll</span><span class="p">())</span>
</pre></div>
</div>
<p>In the case of OpenFOAM simulations, the <em>Popen</em> function is a good candidate to perform individual simulations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_simulation</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Execute a simulation by running the *Allrun* script.</span>

<span class="sd">    :param path: path to simulation folder</span>
<span class="sd">    :type path: str</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">return_code</span> <span class="o">=</span> <span class="n">Popen</span><span class="p">([</span><span class="s2">&quot;./Allrun&quot;</span><span class="p">],</span> <span class="n">cwd</span><span class="o">=</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">return_code</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Simulation </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2"> completed successfully.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: simulation </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2"> failed.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If we had a list of paths to modified simulations, we could loop over the list and use the <em>run_simulation</em> function to execute one simulation after the other. We can accelerate the execution process if we have enough resources to perform multiple simulations at once. Python supports running multiple processes at once via the <em>multiprocessing</em> module. A suitable workflow making use of the <em>run_simulation</em> function defined before, would be to create a process pool based on the number of available CPU cores and to map all the list of all simulation paths to the <em>run_simulation</em> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">simulations</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;./sim_0/&quot;</span><span class="p">,</span> <span class="s2">&quot;./sim_1/&quot;</span><span class="p">,</span> <span class="s2">&quot;./sim_2/&quot;</span><span class="p">]</span>
<span class="n">pool</span> <span class="o">=</span> <span class="n">Pool</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pool</span><span class="p">:</span>
    <span class="n">pool</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">run_simulation</span><span class="p">,</span> <span class="n">simulations</span><span class="p">)</span>
</pre></div>
</div>
<p>Employing the code snippet above, simulations zero and one would be run roughly at the same time. Whichever subprocess finishes first will then execute simulation two.</p>
</section>
<section id="normalizing-data">
<h2>Normalizing data<a class="headerlink" href="#normalizing-data" title="Permalink to this headline">#</a></h2>
<p>Suppose we created a dataset, in which an airfoil’s drag coefficient was determined for several Reynolds numbers <span class="math notranslate nohighlight">\(Re\)</span> and angles of attack <span class="math notranslate nohighlight">\(\alpha\)</span>. The Reynolds number ranges between <span class="math notranslate nohighlight">\(Re=10\times 10^6\)</span> and <span class="math notranslate nohighlight">\(Re=20\times 10^6\)</span>. The angle of attack was varied between <span class="math notranslate nohighlight">\(\alpha = 0^\circ\)</span> and <span class="math notranslate nohighlight">\(\alpha = 4^\circ\)</span>. Now we want to build a surrogate model by means of regression. Based on section 1, it should be clear that combining the features <span class="math notranslate nohighlight">\(Re\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> directly in a model is not a good idea. So, how can we improve the situation? A simple solution is to normalize both features. There are two common ways to normalize any kind of data. In a min-max-scaling, each feature <span class="math notranslate nohighlight">\(x_i\)</span> are scaled to the range <span class="math notranslate nohighlight">\([0,1]\)</span>:
$<span class="math notranslate nohighlight">\(
  \tilde{x}_i = \frac{x_i-x_{i,min}}{x_{i,max}-x_{i,min}}.
\)</span><span class="math notranslate nohighlight">\(
Alternatively, the equation above could also be rescaled to the range \)</span>[-1,1]<span class="math notranslate nohighlight">\(. Min-max scaling works great if the input data are clean, e.g., if there are no outliers. A normalization technique less sensitive to outliers scales the features with their mean \)</span>\mu_x<span class="math notranslate nohighlight">\( and standard deviation \)</span>\sigma_x<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(
  \tilde{x}_i = \frac{x_i-\mu_{x_i}}{\sigma_{x_i}}.
\)</span><span class="math notranslate nohighlight">\(
The re-scaled features have zero mean and unit standard deviation. These properties are also the reason why this scaling is a good choice when training neural networks employing sigmoid and hyperbolic tangents activation functions. These activation functions are not so popular any longer in most fields using deep learning. However, for PINNs they are essential because one can compute arbitrarily many derivatives of both functions. Without going further into detail about neural networks, it is sufficient to now that we do not want activation functions to *saturate*, which mean to become insensitive to the input. Looking at the sigmoid function depicted below, saturation occurs if the argument becomes too small or too low. Data normalized by their mean and standard deviation lead to activation values in the most sensitive range (the slope is the highest around \)</span>x=0$).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the sigmoid function.</span>

<span class="sd">    :param x: input argument</span>
<span class="sd">    :type x: pt.Tensor</span>
<span class="sd">    :return: sigmoid of x</span>
<span class="sd">    :rtype: pt.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">pt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\sigma (x)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\mathrm</span><span class="si">{d}</span><span class="s2">\sigma (x)/\mathrm</span><span class="si">{d}</span><span class="s2"> x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/sigmoid_function.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_8_0.png" src="../../_images/ml_intro_8_0.png" />
</div>
</div>
<p>Clustering is another great example to demonstrate the importance of normalization. Suppose we have the following data points for Reynolds number <span class="math notranslate nohighlight">\(Re\)</span> and lift coefficient <span class="math notranslate nohighlight">\(c_l\)</span>:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><span class="math notranslate nohighlight">\(i\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(Re\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(c_l\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(10\times 10^6\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.3\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>2</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(12\times 10^6\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.12\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(15\times 10^6\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.11\)</span></p></td>
</tr>
</tbody>
</table>
<p>Clustering is about grouping similar data points. Eyeballing the data, you probably see that something significant must have happened in the range <span class="math notranslate nohighlight">\(10\times 10^6 &lt; Re &lt; 12\times 10^6\)</span> because the lift dropped to about one third for the measurement points 2 and 3. If you were tasked to sort similar data points into two different bins, you would probably separate point 1 from the points 2 and 3. To automate the clustering, we need a more precise definition of the notion of similarity we used so far. The most common metric to measure the distance between data points is the <a class="reference external" href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean norm</a>. For the data point above, the distances are:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>points</p></th>
<th class="text-align:center head"><p>distance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1-2</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(2\times 10^6\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>1-3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(5\times 10^6\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>2-3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(3\times 10^6\)</span></p></td>
</tr>
</tbody>
</table>
<p>According to the Euclidean distance, we should rather group points 1 and two together because the distance between them is the smallest. Of course, this outcome is not really what we wanted. Again, normalization can help the situation. The min-max-normalized data are:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><span class="math notranslate nohighlight">\(i\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\tilde{Re}\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\tilde{c}_l\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>2</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.4\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.05\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
</tbody>
</table>
<p>The distances between the points are now:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>points</p></th>
<th class="text-align:center head"><p>distance</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1-2</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1.03\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>1-3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(1.41\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>2-3</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.60\)</span></p></td>
</tr>
</tbody>
</table>
<p>This time, the distance reflects out initial evaluation. The distance between points 2 and 3 is the smallest. In higher dimensions, seeing such normalization-related issue is not straight forward. Therefore, it is good practice to <strong>always</strong> normalize all the data, features as well as labels. In the worst case, normalization costs you a few extra operations to scale and re-scale the model’s input and output. In most cases, normalization improves accuracy and accelerates the training.</p>
<p>Finally, if the data points of a feature are very unevenly distributed in the feature space, it might be sensible to work with the logarithm of the feature instead of the feature itself. An example displaying the usefulness of logarithmic scaling is part of the next lecture.</p>
</section>
<section id="feature-selection-and-design">
<h2>Feature selection and design<a class="headerlink" href="#feature-selection-and-design" title="Permalink to this headline">#</a></h2>
<p>Selecting and designing good features is one of the most essential parts of applying ML to CFD problems. Sometimes, the features and labels are naturally given, and it is straight forward to identify the mapping that the ML model has to perform. Suppose we have a car model and want to create a surrogate model predicting the drag as a function of the Reynolds number based on a couple of CFD simulations. In the simulation, the inlet velocity may be defined to impose a certain Reynolds number und the aerodynamic forces acting on the car result from the velocity and pressure fields. There would be only one feature and one label, and we could choose from a variety of regression algorithms to build a suitable surrogate model. However, sometimes it can be sensible to design additional features based on the extracted raw data. Potential reasons for designing new features could be:</p>
<ul class="simple">
<li><p>to enforce mathematical constraints like symmetry or boundedness</p></li>
<li><p>to leverage known (physical) relations</p></li>
<li><p>to reduce the variance in the data before training</p></li>
</ul>
<p>In summary, one could say that feature design serves the purpose to simplify the ML problem. Good features accelerate the model training and often lead to smaller yet more accurate models. However, care has to be taken when selecting or designing features. It is essential to know right from the beginning what exactly the model is going to be used for in a target application. One common pitfall is basing the model on features that are not (easily) available in the target application. For example, when deriving models for low-fidelity simulations based on high fidelity data, we must first convert the high-fidelity data into features that are also available during the runtime of the low-fidelity simulation. To give a more specific example, direct numerical simulations (DNS) data may be used for turbulence modeling in large eddy simulation (LES) or Reynolds-averaged Navier-Stokes (RANS) simulation approaches. In all approaches, there is going to be a <em>velocity field</em>. However, the mathematical definition of velocity and other quantities varies in each simulation. DNS provides spatially and temporally resolved fields, while LES works with filtered and RANS with time-averaged information. Therefore, the first processing step should be to filter or average the DNS data before exploring and designing new features. Not only the mathematical definition of features but also assumptions about the spatial arrangement might be important. For example, DNS solvers might operate on structured Cartesian meshes, while solver for industrial applications operate on unstructured meshes with arbitrary topology. While it might seem a good idea at first to use specialized neural network architectures like convolutional neural networks to process the array-like DNS data, such a model would be hard or even impossible to apply in the unstructured target solver.</p>
<p>A general rule of thumb for designing additional features is to have rather too many than too few features. Not all features should and will be used for model training since the model may become unnecessary complex or prone to over-fitting the data. However, instead of eliminating features manually, one could consider employing feature exploration and selection strategies. Some examples are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://stackoverflow.com/questions/39409866/correlation-heatmap">correlation heatmaps</a> visualize the linear correlation between different features and labels; if the correlation between a feature and a label is strong, the feature might be a good candidate for modeling</p></li>
<li><p>sequential forward/backward selection performs eliminatory rounds with varying numbers and combinations of features; each round, one feature is removed (backward selection) or added (forward selection)</p></li>
<li><p>decision trees or ensemble variants thereof have a built-in <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">feature importance</a> to rank features (based on the internal criteria at each node of the tree)</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance">permutation feature importance</a> shuffles the values of a single feature randomly and observes how the performance of a training model is affected</p></li>
</ul>
<p>The feature ranking strategies have different pros and cons. Sequential backward selection, for example, yields a meaningful ranking in most cases but can be very expensive to compute. Decision trees, on the other hand, are easily trained, but their <em>feature importance</em> is more susceptible to small changes in the data.</p>
</section>
<section id="deep-learning">
<h2>Deep learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">#</a></h2>
<section id="test-case-overview">
<h3>Test case overview<a class="headerlink" href="#test-case-overview" title="Permalink to this headline">#</a></h3>
<p>To demonstrate the training and evaluation of neural networks, we need some training data. Here, we work with developed velocity profiles of a 1D channel flow simulation. Conducting the parameter study to create the data is part of the accompanying exercise session. The channel flow is characterized by the Reynolds number <span class="math notranslate nohighlight">\(Re=2\delta\bar{U}/\nu\)</span>, where <span class="math notranslate nohighlight">\(\delta\)</span> is one half of the channel height, <span class="math notranslate nohighlight">\(\bar{U}\)</span> is the average velocity along the channel, and <span class="math notranslate nohighlight">\(\nu\)</span> is the kinematic viscosity. In the parameter study, 16 different values <span class="math notranslate nohighlight">\( 10^4 &lt; Re &lt; 10^5\)</span> were sampled with latin hypercube sampling and the corresponding simulations executed. The main output of a single simulation is the velocity along the channel <span class="math notranslate nohighlight">\(u_x\)</span> at several distances <span class="math notranslate nohighlight">\(y\)</span> normal to the channel wall. Our goal is creating a model that provides us with the normalized streamwise velocity <span class="math notranslate nohighlight">\(\tilde{u}_x = u_x/\bar{U}_{max}\)</span> value for a given normalized distance from the wall <span class="math notranslate nohighlight">\(\tilde{y} = y/(2\delta)\)</span> and the Reynolds number:
$<span class="math notranslate nohighlight">\(
  \tilde{u}_{x} = f_\theta(\tilde{y}, Re).
\)</span><span class="math notranslate nohighlight">\(
The features are \)</span>\tilde{y}<span class="math notranslate nohighlight">\( and \)</span>Re<span class="math notranslate nohighlight">\(. The label is \)</span>\tilde{u}_x<span class="math notranslate nohighlight">\(. Such a model could be the basis for improving the accuracy of the near wall region of turbulent flow simulations. Note that there are more clever ways to normalize \)</span>y<span class="math notranslate nohighlight">\( and \)</span>u_x$, but we stick to the version above to make the learning slightly more challenging.</p>
<p>As a first step, we load the velocity profiles, the points normal to the wall at which the velocity values are defined, and the Reynolds numbers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cases</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="s2">&quot;../exercises/boundary_layer_1D_variation/Ub_*&quot;</span><span class="p">)</span>
<span class="n">cases</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">cases</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">case</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">case</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">FOAMDataloader</span><span class="p">(</span><span class="n">cases</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">vertices</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">u_x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">cases</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">case</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cases</span><span class="p">):</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">FOAMDataloader</span><span class="p">(</span><span class="n">case</span><span class="p">)</span>
    <span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load_snapshot</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">loader</span><span class="o">.</span><span class="n">write_times</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">Ubar</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">case</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">case</span> <span class="ow">in</span> <span class="n">cases</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of data matrix: &quot;</span><span class="p">,</span> <span class="n">u_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Could not find precomputed cell centers and volumes.
Computing cell geometry from scratch (slow, not recommended for large meshes).
To compute cell centers and volumes in OpenFOAM, run:

postProcess -func &quot;writeCellCentres&quot; -constant -time none
postProcess -func &quot;writeCellVolumes&quot; -constant -time none
Shape of data matrix:  torch.Size([200, 16])
</pre></div>
</div>
</div>
</div>
<p>There are 200 points normal to the wall and 16 different Reynolds numbers. Each velocity profile contains 200 values and all 16 profiles assembled yield a <span class="math notranslate nohighlight">\(200\times 16\)</span> tensor. Since the data are low-dimensional, we have the luxury of being able to visualize the full dataset all at once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">delta</span><span class="p">,</span> <span class="n">nu</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0e-5</span>
<span class="n">Re</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">Ub</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="o">/</span><span class="n">nu</span> <span class="k">for</span> <span class="n">Ub</span> <span class="ow">in</span> <span class="n">Ubar</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$Re=</span><span class="si">{:1.0f}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">Re</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$u_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper center&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/developed_1d_profile.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_13_0.png" src="../../_images/ml_intro_13_0.png" />
</div>
</div>
<p>The normalization visualized does not alter the nature of the data significantly. However, if <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(u_x\)</span> had very different values ranges, normalizing by the channel height and the average velocity would bring both quantities much closer to the desired range about <span class="math notranslate nohighlight">\(O(1)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$Re=</span><span class="si">{:1.0f}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">Re</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{u}</span><span class="s2">_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper center&quot;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/developed_1d_profile_normalized.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_15_0.png" src="../../_images/ml_intro_15_0.png" />
</div>
</div>
<p>A first inspection of the data shows the expected behavior. As the Reynolds number increases, the average channel velocity increases and the profiles become steeper close to the wall. These observations are a good first sanity check. However, we should validate the data against known results for the same or similar settings. Luckily, the problem is relatively simple such that we can compare the outcome against an existing analytical function. We know that close to the wall, the velocity profile should follow first a linear and then a logarithmic trend. This relation is know as <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_the_wall">law of the wall</a>. Spalding’s function comines both linear and logarithmic zones. However, Spalding’s function is given in so-called inner coordinates:
$<span class="math notranslate nohighlight">\(
  u_\tau = \sqrt{\nu\partial_yu_x|_{y=0}/\rho},\quad y^+ = yu_\tau/\nu,\quad u^+ = u_x/u_\tau.
\)</span><span class="math notranslate nohighlight">\(
The velocity \)</span>u_\tau<span class="math notranslate nohighlight">\( is also called shear stress or friction velocity. Since the simulation is incompressible, we assume a density of unity. Spalding's function is then given as:
\)</span><span class="math notranslate nohighlight">\(
  y^+ = u^+ + \left[e^{u^+\kappa} - 1 - u^+\kappa\left(1+u^+\kappa/2\right) - \left(u^+\kappa\right)^3/6\right] / E,
\)</span><span class="math notranslate nohighlight">\(
where the constants have values of \)</span>E=9.8<span class="math notranslate nohighlight">\( and \)</span>\kappa = 0.41$.</p>
<p>To compare against Spalding’s function, we have to convert the velocity profiles into inner coordinates. The velocity’s slope at the wall <span class="math notranslate nohighlight">\(\partial_yu_x|_{y=0}\)</span> can be approximated for each velocity profile using finite differences and assuming a velocity of zero at the wall. The linear approximation of the slope is reasonable since the boundary layer is fully resolved by the mesh. In the figure below, we can observe how the simulation data nicely collapse almost into a single profile, which is in agreement with Spalding’s function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">spalding</span><span class="p">(</span><span class="n">up</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">E</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">9.8</span><span class="p">,</span> <span class="n">kappa</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.41</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute Spalding&#39;s velocity profile.</span>

<span class="sd">    :param up: velocity in inner coordinates U-plus</span>
<span class="sd">    :type up: pt.Tensor</span>
<span class="sd">    :param E: wall roughness constant, defaults to 9.8</span>
<span class="sd">    :type E: float, optional</span>
<span class="sd">    :param kappa: van Kármán constant, defaults to 0.41</span>
<span class="sd">    :type kappa: float, optional</span>
<span class="sd">    :return: distance in inner coordinates y-plus</span>
<span class="sd">    :rtype: pt.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">upk</span> <span class="o">=</span> <span class="n">up</span><span class="o">*</span><span class="n">kappa</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">upk</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">upk</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">upk</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">upk</span><span class="o">**</span><span class="mi">3</span><span class="o">/</span><span class="mi">6</span>
    <span class="k">return</span> <span class="n">up</span> <span class="o">+</span> <span class="n">temp</span><span class="o">/</span><span class="n">E</span>

<span class="n">dy_u_x</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">-</span> <span class="n">u_x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">U_tau</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">nu</span> <span class="o">*</span> <span class="n">dy_u_x</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">Ub</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Ubar</span><span class="p">):</span>
    <span class="n">y_plus</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="n">U_tau</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">nu</span>
    <span class="n">U_plus</span> <span class="o">=</span> <span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">U_tau</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_plus</span><span class="p">,</span> <span class="n">U_plus</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="n">U_spalding</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">spalding</span><span class="p">(</span><span class="n">U_spalding</span><span class="p">),</span> <span class="n">U_spalding</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Spalding&#39;s function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y^+$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$u^+$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">2500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/1d_profile_inner.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_17_0.png" src="../../_images/ml_intro_17_0.png" />
</div>
</div>
</section>
<section id="simple-feed-forward-neural-networks">
<h3>Simple feed-forward neural networks<a class="headerlink" href="#simple-feed-forward-neural-networks" title="Permalink to this headline">#</a></h3>
<p>Deep learning (DL) is a sub-field of ML that focuses on the design, implementation, and optimization of deep neural networks. Neural networks form a class of parametrized functions characterized by great approximation capabilities, especially if high-dimensional mappings from features to labels are required. The functional form of neural networks is inspired by the <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a>, which was the first mathematical approach to explain the working of neurons (nerve cells) in the brain. The building blocks of modern neural networks still have some resemblance with the original perceptron, but many modifications have been introduced. These modifications are largely driven by empirical observations and mathematical intuition. In this lecture, we introduce the mathematical form of relatively simple feed-forward neural networks with fully connected layers and use them as a black box function approximation tool. In the next lecture, we learn more about the inner workings of neural networks.</p>
<p>The picture below depicts a feed-forward neural network with its typical components: input and output layers, hidden layers, neurons with activations, bias units, and connections between these components.</p>
<img alt="notebooks/ml_comp/media/example_nn.png" src="notebooks/ml_comp/media/example_nn.png" />
<p>In the input layer, one neuron is reserved for each possible feature <span class="math notranslate nohighlight">\(x_i\)</span> of the feature vector <span class="math notranslate nohighlight">\(\mathbf{x} = \left[x_1, x_2, ..., x_{N_f}\right]^T\)</span>, where <span class="math notranslate nohighlight">\(N_f\)</span> is the number of features. The network sketched above has two neurons for two input features, so <span class="math notranslate nohighlight">\(N_f = 2\)</span>. Considering the test case described in the previous section, the two input features <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> would be the normalized channel height and the Reynolds number, and the single output would be the normalized streamwise velocity. The connections between the input and the first hidden layer represent free parameters, also called weights. If each neuron of one layer is connected with each neuron of the consecutive layer, the layers are said to be <em>fully connected</em>. Fully connected layers are part of almost any modern neural network architecture. In every neuron <span class="math notranslate nohighlight">\(j\)</span> of a hidden layer <span class="math notranslate nohighlight">\(l\)</span>, a weighted sum <span class="math notranslate nohighlight">\(z_j^l\)</span> of all <span class="math notranslate nohighlight">\(N_{neu}^{l-1}\)</span> neurons (features) of the previous layer <span class="math notranslate nohighlight">\(l-1\)</span> is computed:
$<span class="math notranslate nohighlight">\(
  z_j^l\left(\mathbf{x}^{l-1}\right) = \sum\limits_{i=1}^{N_{neu}^{l-1}} w_{ji}^{l-1} x_i^{l-1} + b_j^{l-1}.
\)</span><span class="math notranslate nohighlight">\(
The coefficients \)</span>w_{ij}<span class="math notranslate nohighlight">\( of weight matrix \)</span>\mathbf{W}<span class="math notranslate nohighlight">\( are represented by the lines connecting pairs of neurons in the sketch above. \)</span>b_j<span class="math notranslate nohighlight">\( is the bias unit for the \)</span>j<span class="math notranslate nohighlight">\(th neuron in the next layer. In vector-matrix notation, we can write the above as:
\)</span><span class="math notranslate nohighlight">\(
  \mathbf{z}_l = \mathbf{W}_{l-1}^T \mathbf{x}_{l-1} + \mathbf{b}_{l-1},
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\mathbf{z}<em>l<span class="math notranslate nohighlight">\( and \)</span>\mathbf{b}</em>{l-1}<span class="math notranslate nohighlight">\( are a vector of length \)</span>N_{neu}^{l}<span class="math notranslate nohighlight">\(, \)</span>\mathbf{x}<em>{l-1}<span class="math notranslate nohighlight">\( is a vectors of length \)</span>N</em>{neu}^{l-1}<span class="math notranslate nohighlight">\(, and \)</span>\mathbf{W}<em>{l-1}<span class="math notranslate nohighlight">\( is a matrix of shape \)</span>N</em>{neu}^{l-1}\times N_{neu}^l<span class="math notranslate nohighlight">\(. To increase the network's ability of approximating complex functions, the weighted input must be transformed by a non-linear function. Otherwise, the resulting approximating would remain linear with respect to the inputs regardless of the amount of added layers. Motivated by the biological roots of the approach, the non-linear transformation function \)</span>a(z)<span class="math notranslate nohighlight">\( is commonly called activation function. Typical activation functions will be covered further down in this notebook. The weighted sum of the inputs combined with the activation function results in the inputs (features) of the next layer. Therefore, one could interpret the activation's output as new features \)</span>x_j^l<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(
  \mathbf{x}_l = a_l(\mathbf{z}_l) = a_l(\mathbf{W}_{l-1}^T\mathbf{x}_{l-1}+\mathbf{b}_{l-1}).
\)</span><span class="math notranslate nohighlight">\(
The length of the new feature vector corresponds to \)</span>N_{neu}^l<span class="math notranslate nohighlight">\(. Note that the activation function gets a lower script \)</span>l<span class="math notranslate nohighlight">\( to indicate that different activation functions may be used in different layers. All networks created and trained in this lecture employ the same activation functions in all hidden layers but may use an application-specific activation for the output layer. In a general regression task, it is likely that the last activation function is linear, i.e., an identity mapping (\)</span>a(z) = z<span class="math notranslate nohighlight">\(). Following the notation introduced above, we can express a fully-connected feed-forward neural network with two hidden layers and linear (no) activation for the output as:
\)</span><span class="math notranslate nohighlight">\(
  f_\theta(\mathbf{x}) = \mathbf{W}_2^T a_1(\mathbf{W}_1^T a_0(\mathbf{W}_0^T\mathbf{x} + \mathbf{b}_0) + \mathbf{b}_1) + \mathbf{b}_2,
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\theta<span class="math notranslate nohighlight">\( represents all the networks tunable parameters contained in the weight matrices and bias units. The re-combination of the feature vector in each one of the network's layers may be written as a repeated function composition \)</span>f(g(x)) = f\circ g(x)<span class="math notranslate nohighlight">\(. Abbreviating the transformation in a single layer \)</span>l<span class="math notranslate nohighlight">\( as \)</span>m_l(\mathbf{x}<em>l)<span class="math notranslate nohighlight">\(, the full network \)</span>f</em>{\theta}(\mathbf{x})<span class="math notranslate nohighlight">\( may be expressed as:
\)</span><span class="math notranslate nohighlight">\(
  f_{\theta}(\mathbf{x}) = m_{N_L} \circ m_{N_{L-1}}\circ ... \circ m_0 (\mathbf{x}),
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\mathbf{x}<span class="math notranslate nohighlight">\( is the original feature vector provided by the input layer and \)</span>N_L<span class="math notranslate nohighlight">\( is the overall number of layers. Evaluating the model for a concrete feature vector \)</span>\mathbf{x}_i<span class="math notranslate nohighlight">\(, a so-called *forward pass*, yields a prediction, typically denoted by \)</span>\hat{\mathbf{y}}_i<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(
  \hat{\mathbf{y}}_i = f_{\theta}(\mathbf{x}_i).
\)</span><span class="math notranslate nohighlight">\(
Training a neural networks means optimizing its free parameters such that all predictions \)</span>\hat{\mathbf{y}}_i<span class="math notranslate nohighlight">\( match the true labels \)</span>\mathbf{y}_i$ closely. The precise formulation and implementation of the optimization problem follows in the next sections.</p>
<p>The define the network structure discussed before, we can employ <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">Linear</a> modules, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU">ReLU</a> activations, and wrap the layers in a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential">Sequential</a> container, which forwards the output from one layer to the next. Making predictions works by calling the sequential container like a function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">n_neurons</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">simple_network</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">example_input</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_in</span><span class="p">)</span>
<span class="n">simple_network</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.0420], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="preparing-the-datasets">
<h3>Preparing the datasets<a class="headerlink" href="#preparing-the-datasets" title="Permalink to this headline">#</a></h3>
<p>PyTorch comes with several functions and classes that facilitate the creation of datasets. These abstractions are bundled the the subpackage <code class="docutils literal notranslate"><span class="pre">torch.utils.data</span></code>. As a first step, we reshape our data and wrap the features and labels in a <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset">TensorDataset</a>. <code class="docutils literal notranslate"><span class="pre">TensorDatasets</span></code> are special in that the entire data must fit into memory. Keeping the data in memory avoids the relatively slow loading from the hard disk. On the other hand, the size of the dataset is limited by the available memory. For larger datasets, PyTorch provides a general <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset">Dataset</a> class, which prefetches data from disk to reduce waiting times during training.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code> constructor expects feature and label tensors as input. Currently, we have the distance normal to the wall, the Reynolds numbers, and the velocity profiles in separate data structures. However, the data structure needed to train a model <span class="math notranslate nohighlight">\(\tilde{u}_x = f_\theta(\tilde{y}, Re)\)</span> looks as follows:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\tilde{u}_x\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\tilde{y}\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(Re\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.131\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.0001\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(98362\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.201\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(0.0002\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(98362\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>…</p></td>
<td class="text-align:center"><p>…</p></td>
<td class="text-align:center"><p>…</p></td>
</tr>
</tbody>
</table>
<p>The order of the columns is arbitrary and we could also separate the label <span class="math notranslate nohighlight">\(\tilde{u}_x\)</span> from the two features <span class="math notranslate nohighlight">\(\tilde{y}\)</span> and <span class="math notranslate nohighlight">\(Re\)</span>. For <span class="math notranslate nohighlight">\(N_r\)</span> selected Reynolds numbers/profiles and <span class="math notranslate nohighlight">\(N_y\)</span> points per profile, the resulting tensor should have <span class="math notranslate nohighlight">\(N_r\times N_y\)</span> rows and three columns. The <code class="docutils literal notranslate"><span class="pre">reshape_data</span></code> function below creates the required format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reshape_data</span><span class="p">(</span><span class="n">u_x_norm</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_norm</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Re</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create feature and label vectors.</span>

<span class="sd">    :param u_x_norm: velocity profiles normalized with Ubar; first zeroth dimension</span>
<span class="sd">     corresponds to the distance y; first dimension corresponds to the Reynolds number</span>
<span class="sd">    :type u_x_norm: pt.Tensor</span>
<span class="sd">    :param y_norm: distance from the wall normalized by twice the channel hight;</span>
<span class="sd">        ordered from lowest to largest</span>
<span class="sd">    :type y_norm: pt.Tensor</span>
<span class="sd">    :param Re: Reynolds number based on average velocity, twice the channel height,</span>
<span class="sd">        and kinematic viscosity</span>
<span class="sd">    :type Re: pt.Tensor</span>
<span class="sd">    :return: data suitable for training; if there are Ny points normal to</span>
<span class="sd">        the wall and Nr different Reynolds numbers, the resulting tensor</span>
<span class="sd">        has the shape (Ny*Nr, 3)</span>
<span class="sd">    :rtype: pt.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">u_x_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">u_x_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">u_x_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">u_x_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">u_x_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">data</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">u_x_norm</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">data</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_norm</span>
        <span class="n">data</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">Re</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># normalize velocity and wall normal distance</span>
<span class="n">u_x_norm</span> <span class="o">=</span> <span class="n">u_x</span> <span class="o">/</span> <span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">y_norm</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">)</span>
<span class="c1"># reshape data</span>
<span class="n">reshaped_data</span> <span class="o">=</span> <span class="n">reshape_data</span><span class="p">(</span><span class="n">u_x_norm</span><span class="p">,</span> <span class="n">y_norm</span><span class="p">,</span> <span class="n">Re</span><span class="p">)</span>
<span class="c1"># create a TensorDataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">reshaped_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">reshaped_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c1"># the dataset supports indexing; dataset[0] returns the 0th feature-label-pair</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([3.3002e-05, 1.3430e+04]), tensor(0.0002))
</pre></div>
</div>
</div>
</div>
<p>Next, we split the data into three parts. One part contains most of the data and will be used for optimizing the network’s free parameters. The other two parts are significantly smaller and will be used to evaluate the training process. One of the smaller parts is meant to provide feedback during the optimization while the other one is only evaluated after the training is done. In summary, the data is split into:</p>
<ul class="simple">
<li><p><strong>training data</strong>: optimization of model parameters</p></li>
<li><p><strong>validation data</strong>: model evaluation during training</p></li>
<li><p><strong>testing data</strong>: final model evaluation after training</p></li>
</ul>
<p>Note that the naming of testing and validation data is somewhat arbitrary and ML practitioners may use the terms interchangeably. For the low-dimensional problem considered here, we can inspect the model’s properties by means of standard visualization techniques. However, as the problems become high-dimensional, the split is essential to assess the model’s characteristics. More about the assessment follows in a dedicated section of this notebook.</p>
<p>PyTorch’s <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split">random_split</a> divides a dataset according to user-defined sizes. The code below creates three datasets, where the first dataset contains roughly <span class="math notranslate nohighlight">\(70\%\)</span> of the data and the remaining features-label-pairs are equally distributed in validation and testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.7</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.15</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span> <span class="o">-</span> <span class="n">val_size</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">,</span> <span class="n">test_size</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3200, 2240, 480, 480)
</pre></div>
</div>
</div>
</div>
<p>The splitting approach employed before is a good option for general datasets. However, our data comes with a certain structure related to the fact that groups of feature-labels-pairs belong to a single profile/simulation. With the current split, validation and testing data won’t be very different from the training data due the high mesh resolution in <span class="math notranslate nohighlight">\(y\)</span> direction. What we rather want to test with the additional two datasets in the performance at unknown Reynolds numbers. We can build this stricter evaluation approach by splitting the datasets based on individual velocity profiles. The randomly select the profiles/simulations, we use the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html">multinomial</a> probability distribution. The multinomial probability distribution draws samples from discrete events (categories) according to a given list of probabilities for each category. When sampling, the probability distribution returns the indices of the drawn categories. In our case, there are 16 different categories, one for each Reynolds number. The probability tensor is initialized with ones, meaning that every category has the same chance of being drawn. We then draw ten Reynolds numbers for training, set the corresponding indices of the probability tensor to zero, and repeat the process for validation and testing data. It is important to note that, by the implementation’s defaults, no category will be drawn twice because the <code class="docutils literal notranslate"><span class="pre">replacement</span></code> variable is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>A final constrained we apply to the splitting strategy concerns the minimum and maximum Reynolds numbers. We don’t want the model to extrapolate, which is in general something to avoid. Therefore, we enforce the lowest and highest <span class="math notranslate nohighlight">\(Re\)</span> data to be in the training set. This task is easy since the <code class="docutils literal notranslate"><span class="pre">Re</span></code> tensor is already sorted.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Re</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([13429.9990, 20390.0000, 24930.0000, 27810.0020, 33960.0000, 40190.0000,
        44320.0000, 51620.0000, 55090.0000, 63880.0039, 68020.0000, 76740.0000,
        80510.0000, 84810.0000, 93240.0000, 96340.0000])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># select two snapshots for testing</span>
<span class="n">n_Re</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">u_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">probs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">test_idx</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">test_size</span><span class="p">)</span>
<span class="n">probs</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">val_idx</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">val_size</span><span class="p">)</span>
<span class="n">probs</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">train_idx</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">train_size</span><span class="p">)</span>
<span class="n">train_idx</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_Re</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">int64</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Re for testing: &quot;</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">test_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Re for validation: &quot;</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">val_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Re for training: &quot;</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">train_idx</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Re for testing:  tensor([80510., 40190., 20390.])
Re for validation:  tensor([63880.0039, 51620.0000, 76740.0000])
Re for training:  tensor([93240.0000, 55090.0000, 27810.0020, 24930.0000, 33960.0000, 68020.0000,
        84810.0000, 44320.0000, 13429.9990, 96340.0000])
</pre></div>
</div>
</div>
</div>
<p>Before creating the final datasets, we need to normalize the data to avoid roundoff errors. The <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> class defined below wraps around the minimum/maximum values and scaling/rescaling functions. We also created a dataset without normalization to show the importance of feature normalization later on. Note that we create the scaler objects only based on the training data, because we don’t want to use any information from the test data during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MinMaxScaler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class to scale/re-scale data to the range [-1, 1] and back.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span>
        <span class="n">data_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="k">return</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">data_norm</span> <span class="o">-</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">rescale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_norm</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_norm</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="k">return</span> <span class="n">data</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">min</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_tensor</span> <span class="o">=</span> <span class="n">reshape_data</span><span class="p">(</span><span class="n">u_x_norm</span><span class="p">[:,</span> <span class="n">train_idx</span><span class="p">],</span> <span class="n">y_norm</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">train_idx</span><span class="p">])</span>
<span class="n">val_tensor</span> <span class="o">=</span> <span class="n">reshape_data</span><span class="p">(</span><span class="n">u_x_norm</span><span class="p">[:,</span> <span class="n">val_idx</span><span class="p">],</span> <span class="n">y_norm</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">val_idx</span><span class="p">])</span>
<span class="n">test_tensor</span> <span class="o">=</span> <span class="n">reshape_data</span><span class="p">(</span><span class="n">u_x_norm</span><span class="p">[:,</span> <span class="n">test_idx</span><span class="p">],</span> <span class="n">y_norm</span><span class="p">,</span> <span class="n">Re</span><span class="p">[</span><span class="n">test_idx</span><span class="p">])</span>
<span class="n">feature_scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
<span class="n">label_scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">feature_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">label_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">feature_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">label_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span>
    <span class="n">feature_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">label_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">train_dataset_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">val_dataset_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">test_dataset_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2000, 600, 600)
</pre></div>
</div>
</div>
</div>
<p>There is one final useful abstraction for datasets, namely a <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">Dataloader</a>. As you will see in the next section, it is useful loop over the datasets in small junks of feature-label-pairs, so-called batches. The dataloader takes a dataset as input and allows us to loop over such non-overlapping batches (non-overlapping means that a single feature-label-pair cannot be contained in multiple batches). Moreover, we can let the dataloader shuffle the batches such that the batches are different each time we loop over the dataset (we don’t want the order of the data in the set to affect the training).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([500, 2]) torch.Size([500, 1])
torch.Size([500, 2]) torch.Size([500, 1])
torch.Size([500, 2]) torch.Size([500, 1])
torch.Size([500, 2]) torch.Size([500, 1])
</pre></div>
</div>
</div>
</div>
</section>
<section id="generic-training-loop">
<h3>Generic training loop<a class="headerlink" href="#generic-training-loop" title="Permalink to this headline">#</a></h3>
<p>To simplify notation, we define a long vector <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> holding all of the network’s parameters (weights). For optimizing the wights, we need a metric expressing how good the model’s prediction is. The most common metric used for regression problems is the mean squared error (MSE), also referred to as <span class="math notranslate nohighlight">\(L_2\)</span> norm/loss. For a set of <span class="math notranslate nohighlight">\(N\)</span> generic pairs of feature vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> and label vectors <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>, the <span class="math notranslate nohighlight">\(L_2\)</span> loss is defined as:
$<span class="math notranslate nohighlight">\(
  L_2(\mathbf{\mathbf{\theta}}) = \frac{1}{N}\sum\limits_{i=1}^N \left( \mathbf{y}_i - f_{\mathbf{\theta}}(\mathbf{x}_i) \right)^2,
\)</span><span class="math notranslate nohighlight">\(
where \)</span>f_{\mathbf{\theta}}(\mathbf{x})<span class="math notranslate nohighlight">\( is the model with weights \)</span>\mathbf{\theta}<span class="math notranslate nohighlight">\( and \)</span> \hat{\mathbf{y}}<em>i = f</em>{\mathbf{\theta}}(\mathbf{x}_i) <span class="math notranslate nohighlight">\( is the prediction for the \)</span>i<span class="math notranslate nohighlight">\(th feature vector instance. The goal of the parameter optimization/training is to find the parameter configuration \)</span>\mathbf{\theta}^\ast<span class="math notranslate nohighlight">\( yielding the minimal loss:
\)</span><span class="math notranslate nohighlight">\(
  \mathbf{\theta}^\ast = \underset{\mathbf{\theta}}{\mathrm{argmin}} L_2 (\mathbf{\theta}).
\)</span>$</p>
<p>The most common update rule for the model weights is called gradient decent. The idea is simple: we compute the gradient of the loss function with respect to the weights and make a small adjustment of the weights in the negative gradient direction:
$<span class="math notranslate nohighlight">\(
  \mathbf{\theta}_{n+1} = \mathbf{\theta}_n - \lambda_0 \frac{\mathrm{d}L}{\mathrm{d}\mathbf{\theta}},
\)</span><span class="math notranslate nohighlight">\(
where \)</span>n<span class="math notranslate nohighlight">\( indicate the current iteration, \)</span>\lambda_0<span class="math notranslate nohighlight">\( is the learning rate, and \)</span>L<span class="math notranslate nohighlight">\( represents a generic loss function. Modern training approaches use a more advanced version of gradient decent that may be expressed as:
\)</span><span class="math notranslate nohighlight">\(
  \mathbf{\theta}_{n+1} = \mathbf{\theta}_n - g\left(\lambda_0,\frac{\mathrm{d}L}{\mathrm{d}\mathbf{\theta}}\right),
\)</span><span class="math notranslate nohighlight">\(
where \)</span>g$ is a functional abstraction of techniques like momentum, learning rate scheduling, and gradient clipping. The first two of these techniques are introduced and implemented later on in this notebook.</p>
<p>In general, the loss function of a neural network may be non-linear and non-convex. Therefore, the loss values do not always decreases monotonically. If we perform a fixed number of optimization epochs/iterations, the final parameter configuration is not necessarily the best one. A simple strategy to keep the best model is to compare the current loss value against the best loss values encountered during the training. If a new best loss is reached, a snapshot of the model is saved. After the training, we can load and evaluate only the best model.</p>
<p>The code block below implements a relatively comprehensive training loop, which will be used for almost all examples in the course involving neural networks. A few important features of the <code class="docutils literal notranslate"><span class="pre">train_model</span></code> function are:</p>
<ul class="simple">
<li><p>evaluation of user-defined metric on training, validation, and test data</p></li>
<li><p>monitoring of training performance in terms of loss and training time</p></li>
<li><p>saving of checkpoints; the checkpoints allow to restart the training, e.g., after an unexpected failure</p></li>
<li><p>saving of the best model based on the training or validation loss</p></li>
<li><p>learning rate adjustment based on a user-defined scheduler</p></li>
</ul>
<p>Only some of these features are explored in this notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_epoch</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">data_loader</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
    <span class="n">loss_func</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">results</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">score_funcs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform one optimizing step on a model.</span>

<span class="sd">    This loop is a slightly modified version of &#39;run_epoch&#39;</span>
<span class="sd">    provided in chapter 5 of &#39;Inside Deep Learning&#39; by Edward Raff;</span>
<span class="sd">    refer to:</span>
<span class="sd">    https://github.com/EdwardRaff/Inside-Deep-Learning/blob/main/idlmam.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># keeping track of loss, predictions, and time</span>
    <span class="n">running_loss</span><span class="p">,</span> <span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

    <span class="c1"># loop over all batches</span>
    <span class="k">for</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">running_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># the dataset might get shuffled in the next loop</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">score_funcs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">labels_true</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">labels_pred</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

    <span class="c1"># keep track of performance</span>
    <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">running_loss</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">running_loss</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">score_funcs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>


<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">loss_func</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">train_loader</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
    <span class="n">val_loader</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test_loader</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">score_funcs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">checkpoint_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">log_all</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lr_schedule</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">_LRScheduler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform one optimizing step on a model.</span>

<span class="sd">    This function is a slightly modified version of &#39;train_network&#39;</span>
<span class="sd">    provided in chapter 5 of &#39;Inside Deep Learning&#39; by Edward Raff;</span>
<span class="sd">    refer to:</span>
<span class="sd">    https://github.com/EdwardRaff/Inside-Deep-Learning/blob/main/idlmam.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># dictionary for keeping track of training performance</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
    <span class="n">ref_loss</span> <span class="o">=</span> <span class="s2">&quot;train_loss&quot;</span> <span class="k">if</span> <span class="n">val_loader</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;val_loss&quot;</span>

    <span class="c1"># use AdamW as default optimizer if none specified</span>
    <span class="n">delete_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">delete_optimizer</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">total_train_time</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># model update</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">total_train_time</span> <span class="o">+=</span> <span class="n">run_epoch</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
            <span class="n">results</span><span class="p">,</span> <span class="n">score_funcs</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span>
        <span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;total_time&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_train_time</span><span class="p">)</span>
        <span class="n">message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Training loss: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">2.6e</span><span class="si">}</span><span class="s2">&quot;</span>


        <span class="c1"># validation dataset</span>
        <span class="k">if</span> <span class="n">val_loader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">pt</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">run_epoch</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                    <span class="n">results</span><span class="p">,</span> <span class="n">score_funcs</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span>
                <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;; Validation loss: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">2.6e</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># update of learning rate</span>
        <span class="k">if</span> <span class="n">lr_schedule</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">):</span>
                <span class="n">lr_schedule</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr_schedule</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># test dataset</span>
        <span class="k">if</span> <span class="n">test_loader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">pt</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">run_epoch</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                    <span class="n">results</span><span class="p">,</span> <span class="n">score_funcs</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;test&quot;</span>
                <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;; Test loss: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;test_loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">2.6e</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># save checkpoint</span>
        <span class="k">if</span> <span class="n">checkpoint_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">suffix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;_epoch_</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">log_all</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
            <span class="n">pt</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;epoch&quot;</span> <span class="p">:</span> <span class="n">e</span><span class="p">,</span>
                    <span class="s2">&quot;model_state_dict&quot;</span> <span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s2">&quot;results&quot;</span> <span class="p">:</span> <span class="n">results</span>
                <span class="p">},</span> <span class="n">checkpoint_file</span> <span class="o">+</span> <span class="n">suffix</span>
            <span class="p">)</span>
            <span class="n">latest_loss</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">ref_loss</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">latest_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">latest_loss</span>
                <span class="n">copy</span><span class="p">(</span><span class="n">checkpoint_file</span> <span class="o">+</span> <span class="n">suffix</span><span class="p">,</span> <span class="n">checkpoint_file</span> <span class="o">+</span> <span class="s2">&quot;_best&quot;</span><span class="p">)</span>
                

        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">e</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> - &quot;</span> <span class="o">+</span> <span class="n">message</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
        <span class="p">)</span>

    <span class="c1"># if the optimizer was created in the training loop,</span>
    <span class="c1"># delete if to avoid unwanted side effects</span>
    <span class="k">if</span> <span class="n">delete_optimizer</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">optimizer</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The function below simplifies the construction of simple fully-connected neural networks with a variable of hidden layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_simple_network</span><span class="p">(</span><span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
        <span class="n">activation</span><span class="p">()</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_0_base</span> <span class="o">=</span> <span class="n">create_simple_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">))</span>
<span class="n">score_funcs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Lmax&quot;</span> <span class="p">:</span> <span class="n">metrics</span><span class="o">.</span><span class="n">max_error</span><span class="p">,</span>
    <span class="s2">&quot;L1&quot;</span> <span class="p">:</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">,</span>
    <span class="s2">&quot;R2&quot;</span> <span class="p">:</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_0</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">results_0</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_0</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
    <span class="n">score_funcs</span><span class="p">,</span>
    <span class="mi">500</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_0</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 3.703182e-03; Validation loss: 3.137902e-03; Test loss: 2.313281e-03
</pre></div>
</div>
</div>
</div>
<p>Plotting the MSE loss indicates a steady learning. With more epochs, the loss is likely to drop further. Moreover, the loss computed on validation and test data shows a similar trend as on the training data. Therefore, the model is most likely not overfitting (memorizing) the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;loss_lecture_4_model_0.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_41_0.png" src="../../_images/ml_intro_41_0.png" />
</div>
</div>
<p>Normalizing the data is enormously important to mitigate the influence of roundoff errors. To demonstrate the impact of normalization, we repeat the training with the unscaled data. The enormous difference in the order of magnitude between <span class="math notranslate nohighlight">\(\tilde{y}\)</span> and <span class="math notranslate nohighlight">\(Re\)</span> inhibits the learning after about 150 epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset_raw</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset_raw</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">val_loader_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset_raw</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataset_raw</span><span class="p">))</span>
<span class="n">test_loader_raw</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset_raw</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset_raw</span><span class="p">))</span>
<span class="n">model_0_raw</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">results_0_raw</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_0_raw</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader_raw</span><span class="p">,</span> <span class="n">val_loader_raw</span><span class="p">,</span> <span class="n">test_loader_raw</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_0_raw</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 3.648483e-02; Validation loss: 3.476597e-02; Test loss: 2.301314e-02
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">results_0_raw</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;loss_lecture_4_model_0_raw.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_44_0.png" src="../../_images/ml_intro_44_0.png" />
</div>
</div>
<p>Comparing the prediction of the first model against the true labels shows that the model reflect the trend of with respect to the Reynolds number already quite well. Some fluctuations of the predicted profiles around the true ones are visible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">label_scaler</span><span class="o">.</span><span class="n">rescale</span><span class="p">(</span>
        <span class="n">model</span><span class="p">(</span><span class="n">feature_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train_0</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_val_0</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_test_0</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>


<span class="n">n_points</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_train_0</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_val_0</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_test_0</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{u}</span><span class="s2">_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;prediction_lecture_4_model_0.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_47_0.png" src="../../_images/ml_intro_47_0.png" />
</div>
</div>
</section>
<section id="training-with-batches-of-data">
<h3>Training with batches of data<a class="headerlink" href="#training-with-batches-of-data" title="Permalink to this headline">#</a></h3>
<p>So far, we used the entire training dataset at once to update the weights in a single epoch. Instead, we can also create small junks of data and update the weights based on each individual junk. This idea is called batch training and leads many more updates within a single epoch. Moreover, computing the gradient based on a subset of the data sometimes helps to overcome local minima and reduces the memory requirements of the training. A drawback of decreasing the batch size, i.e., creating smaller junks, is the increased training time because smaller batches don’t allow to exploit parallelization to its full potential.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
<span class="n">batch_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">:</span>
    <span class="n">model_bs</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
    <span class="n">batch_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">train_model</span><span class="p">(</span>
            <span class="n">model_bs</span><span class="p">,</span>
            <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
            <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_bs</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 3.650685e-03
</pre></div>
</div>
</div>
</div>
<p>In the plot below we see that the loss drops significantly faster for smaller batches and reaches also lower final loss values. However, also the loss fluctuations increase as the gradient prediction becomes more spurious.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">bs</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="n">batch_results</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">bs</span><span class="si">:</span><span class="s2">d</span><span class="si">}</span><span class="s2">, T=</span><span class="si">{</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;total_time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">2.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;batch_size_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_51_0.png" src="../../_images/ml_intro_51_0.png" />
</div>
</div>
</section>
<section id="learning-rate-adjustment">
<h3>Learning rate adjustment<a class="headerlink" href="#learning-rate-adjustment" title="Permalink to this headline">#</a></h3>
<p>One way to counteract oscillations in the training loss is an adjustment of the learning rate. Smaller learning lead to smaller changes in the weights. However, if the learning rate is too small, we may never reach a good optimum or require an excessive amount of epochs/resources. In the learning rate test below, a value of <span class="math notranslate nohighlight">\(\lambda_0 = 5\times 10^{-4}\)</span> yields the best final loss value. Nonetheless, higher learning rates yield a much stronger drop in the initial epochs. Next, we combine the best attributes of low and high learning rates by employing dynamic learning rate adjustment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_to_test</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0e-2</span><span class="p">,</span> <span class="mf">5.0e-3</span><span class="p">,</span> <span class="mf">1.0e-3</span><span class="p">,</span> <span class="mf">5.0e-4</span><span class="p">,</span> <span class="mf">1.0e-4</span><span class="p">]</span>
<span class="n">lr_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">lr_to_test</span><span class="p">:</span>
    <span class="n">model_lr</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
    <span class="n">lr_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">train_model</span><span class="p">(</span>
            <span class="n">model_lr</span><span class="p">,</span>
            <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
            <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_lr</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 2.593159e-03; Validation loss: 2.350085e-03; Test loss: 2.139329e-03
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">lr_to_test</span><span class="p">,</span> <span class="n">lr_results</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lr</span><span class="si">:</span><span class="s2">1.0e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;lr_bs128_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_54_0.png" src="../../_images/ml_intro_54_0.png" />
</div>
</div>
<p>In the cells below, several scheduling strategies for the learning rate are compared:</p>
<ul class="simple">
<li><p><strong>const.</strong>: constant learning rate as initialized in the optimizer</p></li>
<li><p><strong>exp. decay:</strong> exponential decay from the initial to the minimum value within a given number of epochs</p></li>
<li><p><strong>step decay:</strong> learning rate reduction every <span class="math notranslate nohighlight">\(S\)</span> steps</p></li>
<li><p><strong>cosine:</strong> oscillatory adjustment of the learning rate around the initial value; might help to overcome local minima</p></li>
<li><p><strong>plateau:</strong> reduce learning rate by a given factor whenever the validation loss did not decrease for <code class="docutils literal notranslate"><span class="pre">patience</span></code> steps</p></li>
</ul>
<p>The stepwise, exponential, and plateau reduction yield improved final loss value, accelerate the learning, and mitigate loss fluctuations towards the end of the training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_exp_decay</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_exp_decay</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gamma_exp</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0e-4</span> <span class="o">/</span> <span class="mf">1.0e-2</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">500</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma_exp</span><span class="p">)</span>

<span class="n">results_exp_decay</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_exp_decay</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 3.927640e-05; Validation loss: 1.669829e-04; Test loss: 1.406699e-04
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_step_decay</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_step_decay</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gamma_step</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">500</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma_step</span><span class="p">)</span>

<span class="n">results_step_decay</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_step_decay</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 2.291339e-04; Validation loss: 5.352701e-04; Test loss: 5.303427e-04
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_cosine</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_cosine</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">500</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>

<span class="n">results_cosine</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_cosine</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 9.224808e-05; Validation loss: 5.831620e-04; Test loss: 3.860441e-04
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_plateau</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0_base</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_plateau</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
<span class="n">results_plateau</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model_plateau</span><span class="p">,</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 7.438762e-05; Validation loss: 5.873441e-04; Test loss: 1.206016e-04
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scheduler_res</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;const.&quot;</span> <span class="p">:</span> <span class="n">lr_results</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
    <span class="s2">&quot;exp. decay&quot;</span> <span class="p">:</span> <span class="n">results_exp_decay</span><span class="p">,</span>
    <span class="s2">&quot;step decay&quot;</span> <span class="p">:</span> <span class="n">results_step_decay</span><span class="p">,</span>
    <span class="s2">&quot;cosine&quot;</span> <span class="p">:</span> <span class="n">results_cosine</span><span class="p">,</span>
    <span class="s2">&quot;plateau&quot;</span> <span class="p">:</span> <span class="n">results_plateau</span>
<span class="p">}</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">scheduler_res</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;lr_scheduler_bs128_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_60_0.png" src="../../_images/ml_intro_60_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">scheduler_res</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;test data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;lr_scheduler_bs128_test_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_61_0.png" src="../../_images/ml_intro_61_0.png" />
</div>
</div>
<p>The prediction below shows improved accuracy and reduced oscillations compared to the first attempt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train_pl</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_plateau</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_val_pl</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_plateau</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_test_pl</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model_plateau</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>


<span class="n">n_points</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_train_pl</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_val_pl</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_test_pl</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{u}</span><span class="s2">_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;prediction_lecture_4_model_pl.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_63_0.png" src="../../_images/ml_intro_63_0.png" />
</div>
</div>
</section>
<section id="activation-functions">
<h3>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h3>
<p>An essential element of neural networks are the activation functions applied to the weighted sum at each neuron. There are a few heuristic criteria for suitable activation function:</p>
<ul class="simple">
<li><p>nonlinear</p></li>
<li><p>continuous with infinite support</p></li>
<li><p>monotonic</p></li>
<li><p>constant slope</p></li>
<li><p>effectively computable</p></li>
</ul>
<p>There is no single activation function suiting all of the above requirements at once. The activation functions of hidden layers should be always nonlinear, because otherwise, the fully connected layers could be condensed into a single layers. Only the output layer might employ an identity activation, e.g., for regression problems. For classification, there are specialized activation functions converting the input into a probability or the logarithm thereof. The most common activation functions for hidden layers are depicted below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activations</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;sigmoid&quot;</span> <span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
    <span class="s2">&quot;tanh&quot;</span> <span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
    <span class="s2">&quot;ReLU&quot;</span> <span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="s2">&quot;ELU&quot;</span> <span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">,</span>
    <span class="s2">&quot;SELU&quot;</span> <span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span>
<span class="p">}</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">marker</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&gt;&quot;</span><span class="p">]</span>
<span class="n">ls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>  <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$z$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$a(z)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/activation_functions.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_65_0.png" src="../../_images/ml_intro_65_0.png" />
</div>
</div>
<p>For many problems, the rectified linear unit (ReLU) leads to fast and accurate models. In general, it is fair to say that training with exponential linear units (ELU), scaled exponential linear units (SELU), and ReLU converges significantly faster than with sigmoid. For a mathematical definition of these function, refer to <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function">this article</a>. On the other hand, functions with higher smoothness also lead to models with higher smoothness. Models employing ReLU may have kinks and non-constant derivatives. However, ReLUs are also significantly faster to compute since they do not contain the exponential function. The experiment below compares the time needed to evaluate several different activation functions. The hyperbolic tangents function takes the most time to be evaluated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arg</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> -n 10000 -r 10 pt.sigmoid(arg) 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13.5 µs ± 192 ns per loop (mean ± std. dev. of 10 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> -n 10000 -r 10 pt.tanh(arg)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>28.8 µs ± 301 ns per loop (mean ± std. dev. of 10 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> -n 10000 -r 10 pt.nn.functional.elu(arg)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13.5 µs ± 69.7 ns per loop (mean ± std. dev. of 10 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> -n 10000 -r 10 pt.nn.functional.relu(arg)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.34 µs ± 86.7 ns per loop (mean ± std. dev. of 10 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> -n 10000 -r 10 pt.nn.functional.selu(arg)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13.1 µs ± 278 ns per loop (mean ± std. dev. of 10 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>You might be wondering, if the difference of a few micro seconds really matters. Recall that we have to evaluate the activation function for each neuron. Networks may have a few thousand or several billion neurons.  Moreover, we have to evaluate all neurons for each data point during a forward pass. Therefore, similar statistics as the ones above may be found for the overall training time (assuming a constant number of optimization loops).</p>
<p>Another important attribute of an activation function is its derivative with respect to the input. The main reason for the effectiveness of ELU/ReLU/SELU is their constant slope for positive inputs. The constant slope mitigates the problem of vanishing or exploding gradients during the training. For very large networks, it might happen that many neurons employing ReLU become inactive during the training because the slope becomes zero once the input becomes negative. To provide the chance for such neurons to recover, ELU and similar variants were invented. ELU has a small but non-vanishing slope for negative inputs. The reason why training with sigmoid or hyperbolic tangents is usually more difficult lies in their vanishing sensitivity for very large or small arguments. For a long time, this insensitivity made it practically impossible to train networks with many hidden layers because there was no way to control the range of the neurons inputs in the hidden layers. The effect is similar to training with unscaled data. In recent years, a technique called <a class="reference external" href="https://arxiv.org/abs/1502.03167">batch normalization</a> was developed to mitigate the shift of the transformed features in the hidden layers. The SELU function has a similar effect as batch normalization but requires no additional weights.</p>
<p>As a side note, training approaches like PINNs require the computation of higher-order derivatives. Therefore, sigmoid or hyperbolic tangents and all the associated drawbacks are basically the only option.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">marker</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&gt;&quot;</span><span class="p">]</span>
<span class="n">ls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">]</span>
<span class="n">cont</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span>
<span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;C</span><span class="si">{:1d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">activations</span><span class="p">))]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">x_num</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">grad_num</span> <span class="o">=</span>  <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">cont</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_num</span><span class="p">,</span> <span class="n">grad_num</span><span class="p">,</span>
            <span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">xi</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x_num</span> <span class="k">if</span> <span class="n">xi</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">dx</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">dx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_num</span><span class="p">,</span> <span class="n">grad_num</span><span class="p">)</span> <span class="k">if</span> <span class="n">xi</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">],</span>
                <span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">xi</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x_num</span> <span class="k">if</span> <span class="n">xi</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="n">dx</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">dx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_num</span><span class="p">,</span> <span class="n">grad_num</span><span class="p">)</span> <span class="k">if</span> <span class="n">xi</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">],</span>
                <span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$z$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\mathrm</span><span class="si">{d}</span><span class="s2">_za(z)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/activation_slopes.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_74_0.png" src="../../_images/ml_intro_74_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_act</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">models_act</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="n">activations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_simple_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">act</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
    <span class="n">results_act</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span>
        <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>
    <span class="p">)</span>
    <span class="n">models_act</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 5.979562e-05; Validation loss: 5.140446e-05; Test loss: 7.680662e-05
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results_act</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;activations_plateau_bs128_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_76_0.png" src="../../_images/ml_intro_76_0.png" />
</div>
</div>
<p>The plot below shows the model employing hyperbolic tangents activations. The accuracy is improved even further. Moreover, all profiles are significantly smoother.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models_act</span><span class="p">[</span><span class="s2">&quot;tanh&quot;</span><span class="p">]</span>
<span class="n">pred_train</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_val</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>


<span class="n">n_points</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_train</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_val</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{u}</span><span class="s2">_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;prediction_act_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_78_0.png" src="../../_images/ml_intro_78_0.png" />
</div>
</div>
</section>
<section id="advanced-building-blocks">
<h3>Advanced building blocks<a class="headerlink" href="#advanced-building-blocks" title="Permalink to this headline">#</a></h3>
<p>In the next cells, we test a few advanced network building blocks, namely:</p>
<ul class="simple">
<li><p>normalization: normalization layers aim to improve learning by re-scaling the output of the precessing layer; batch normalization learns normalization across batches, while layer normalization learns normalizing the neuron output within a layer</p></li>
<li><p>skip connections: skipping connections means that the input of an early layer in the network is also provided to layer following later, thereby skipping one or more layers in-between; the inputs are merged by concatenation</p></li>
<li><p>residual blocks: similar to skip connections, residual blocks merge the input of a layer with its output; however, the input for the next layer is created by addition instead of concatenation</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_normalized_network</span><span class="p">(</span><span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">layer_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">layer_norm</span><span class="p">:</span>
        <span class="n">norm_layer</span><span class="p">,</span> <span class="n">norm_size</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="p">[</span><span class="n">n_neurons</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">norm_layer</span><span class="p">,</span> <span class="n">norm_size</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span> <span class="n">n_neurons</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
        <span class="n">activation</span><span class="p">(),</span>
        <span class="n">norm_layer</span><span class="p">(</span><span class="n">norm_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">norm_size</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SkipBlock</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
                 <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SkipBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_1</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_2</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_1</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_in</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_2</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_out</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_1</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="n">n_in</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_2</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="n">n_out</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_1</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_norm_2</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layer_2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_skip_network</span><span class="p">(</span><span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">SkipBlock</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SkipBlock</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;batch&quot;</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_out</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="s2">&quot;layer&quot;</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="n">n_out</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_res_network</span><span class="p">(</span><span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">activation</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">),</span>
        <span class="n">activation</span><span class="p">()</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_advanced_models</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;ref.&quot;</span> <span class="p">:</span> <span class="n">create_simple_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">),</span>
        <span class="s2">&quot;batch norm.&quot;</span> <span class="p">:</span> <span class="n">create_normalized_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
        <span class="s2">&quot;layer norm.&quot;</span> <span class="p">:</span> <span class="n">create_normalized_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;skip conn.&quot;</span> <span class="p">:</span> <span class="n">create_skip_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="s2">&quot;skip conn., layer norm.&quot;</span> <span class="p">:</span> <span class="n">create_skip_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="s2">&quot;layer&quot;</span><span class="p">),</span>
        <span class="s2">&quot;res. block&quot;</span> <span class="p">:</span> <span class="n">create_res_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="s2">&quot;res. block, layer norm.&quot;</span> <span class="p">:</span> <span class="n">create_res_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="s2">&quot;layer&quot;</span><span class="p">)</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_adv</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">generate_advanced_models</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
    <span class="n">results_adv</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span>  <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
                                    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 5.116301e-05; Validation loss: 2.753079e-05; Test loss: 5.703035e-05
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results_adv</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Validation loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;adv_networks_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_87_0.png" src="../../_images/ml_intro_87_0.png" />
</div>
</div>
</section>
<section id="dealing-with-uncertainty">
<h3>Dealing with uncertainty<a class="headerlink" href="#dealing-with-uncertainty" title="Permalink to this headline">#</a></h3>
<p>The training of neural networks is not deterministic. If you go back to any cell in which the model training was performed and execute the cell several times, every time you will end up with a different model and a different loss behavior. The reason for this observation is twofold:</p>
<ol class="simple">
<li><p>the loss function is non-convex</p></li>
<li><p>the initial network parameters are sampled randomly</p></li>
</ol>
<p>Basically, we end up in different local minima in each training run. Fortunately, the situation is not as bad as it seems, because neural networks tend to have many local minima yielding good models. Nonetheless, if we want to rigorously compare two different neural networks, we have to deal with the uncertainty of the training as we would it in a real experiment. We repeat the training several times and compare different models on a statistical basis, e.g., by comparing mean, median, or standard deviation over all runs. In the example below, we compare several network designs based on the best <span class="math notranslate nohighlight">\(L_2\)</span> loss. To keep the training time reasonable each training is repeated ten times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_repeat</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">results_repeat</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_repeat</span><span class="p">):</span>
    <span class="n">pt</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">generate_advanced_models</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
        <span class="n">results_repeat</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
                                   <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 5.767075e-05; Validation loss: 6.188699e-05; Test loss: 3.591628e-05
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results_repeat</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
        <span class="p">[</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results_repeat</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">positions</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">flierprops</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;markersize&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s2">&quot;markeredgecolor&quot;</span><span class="p">:</span> <span class="s2">&quot;C3&quot;</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">((</span><span class="s2">&quot;ref.&quot;</span><span class="p">,</span> <span class="s2">&quot;batch</span><span class="se">\n</span><span class="s2">norm.&quot;</span><span class="p">,</span> <span class="s2">&quot;layer</span><span class="se">\n</span><span class="s2">norm.&quot;</span><span class="p">,</span> <span class="s2">&quot;skip</span><span class="se">\n</span><span class="s2">conn.&quot;</span><span class="p">,</span> <span class="s2">&quot;skip+ln&quot;</span><span class="p">,</span> <span class="s2">&quot;res.</span><span class="se">\n</span><span class="s2">block&quot;</span><span class="p">,</span> <span class="s2">&quot;res+ln&quot;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE (test)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;uncertainty_arch_lecture_4.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_90_0.png" src="../../_images/ml_intro_90_0.png" />
</div>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Box_plot">Box plots</a> present a very rich visualization of groups of data. In the figure above, we compare the box plots resulting from the different network architectures. Box plots mainly work with quantiles. The <span class="math notranslate nohighlight">\(p\)</span>th quantile, denoted by <span class="math notranslate nohighlight">\(q_p\)</span>, is obtained by sorting the data and then counting through the data until <span class="math notranslate nohighlight">\(p\%\)</span> of the overall number of data points are reached. For example, the <span class="math notranslate nohighlight">\(50\)</span> th quantile of the set <span class="math notranslate nohighlight">\(\{ 3, 1, 5, 4, 2\}\)</span> would be <span class="math notranslate nohighlight">\(q_{50} = 2\)</span>. The <span class="math notranslate nohighlight">\(50\%\)</span> quantile is commonly called median. In a box plot, the lower and upper boundaries of the box are given by the <span class="math notranslate nohighlight">\(25\)</span>th and <span class="math notranslate nohighlight">\(75\)</span>th quantiles, respectively. The orange line in the plot below indicates the median. Quantiles are also used to identify outliers. The inter quantile range (IQR) is defined as <span class="math notranslate nohighlight">\(q_r = q_{75} - q_{25}\)</span>. Outlier detection based on the IQR assumes that values in the range <span class="math notranslate nohighlight">\(q_{25}-kq_r \le x \le q_{75}+kq_r\)</span> are regular values, whereas values outside this range are outliers. The factor <span class="math notranslate nohighlight">\(k\)</span> is often chosen as <span class="math notranslate nohighlight">\(k=1.5\)</span>. Outliers in the plot below are indicated by red circles. The so-called whiskers (the little antennas extending from the box) indicate the largest and smallest values not considered to be outliers. From box plots, we can infer the spread of the data and also how it is distributed. For example, if the whiskers are symmetric and the median is in the middle of the box, the data are normally distributed (approximately).</p>
<p>In ML research, we want our experiments to be repeatable, even if random sampling is part of the process. Luckily, there are no true random numbers in computer programs. Instead, they are said to be <em>pseudo random</em>, because we can define a so-called <em>seed</em> parameter that makes the sampling repeatable. In the loop of the code cell above, we use the loop count as seed. Therefore, the initial weights change for each training run in a repeatable manner.</p>
<p>An important design parameter to change the model’s flexibility (the ability to fit data) is the number of hidden layers. In the code cell below, we conduct a similar experiment as before but vary only the number of hidden layers. As can be seen in the box plots, the loss and also the spread of the loss decreases significantly as the number of hidden layers increases. Moreover, the improvement in the <span class="math notranslate nohighlight">\(L_2\)</span> loss seems to become smaller and smaller with each additional hidden layers.</p>
</section>
<section id="visualizing-prediction-errors">
<h3>Visualizing prediction errors<a class="headerlink" href="#visualizing-prediction-errors" title="Permalink to this headline">#</a></h3>
<p>As mentioned before, it is relatively easy to assess the model’s behavior for the present problem. For high-dimensional parameter spaces, the evaluation is less straightforward, and we have to rely on statistics and other less intuitive means of visualization. Two techniques are demonstrated below:</p>
<ul class="simple">
<li><p>a histogram of the prediction error; the histogram shows how the error is distributed; ideally, all prediction errors should be located in a narrow band around zero; we can immediately read the maximum prediction errors and infer how many predictions yield such high errors</p></li>
<li><p>a heatmap with the maximum prediction errors in different sub-sections of the feature space; if we had more features, we would create the same plot for the two most important features, or we would create several heatmaps for different combinations of features; the heatmap indicates which sections of the feature space yield high prediction errors; in the example below we see that prediction errors are high for small distances and large Reynolds numbers</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">final_model</span> <span class="o">=</span> <span class="n">create_skip_network</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">final_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-2</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1.0e-4</span><span class="p">)</span>
<span class="n">final_results</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(),</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span>
                            <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">lr_schedule</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Epoch  499/499 - Training loss: 8.186048e-06; Validation loss: 6.622953e-06; Test loss: 7.083297e-06
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train_final</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_val_final</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>
<span class="n">pred_test_final</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">feature_scaler</span><span class="p">,</span> <span class="n">label_scaler</span><span class="p">)</span>


<span class="n">n_points</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Re</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">Ubar</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">delta</span><span class="p">),</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_train_final</span><span class="p">,</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_val_final</span><span class="p">,</span> <span class="n">val_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred_test_final</span><span class="p">,</span> <span class="n">test_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C2&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;testing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{u}</span><span class="s2">_x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;final_prediction_lecture_4_model_pl.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_95_0.png" src="../../_images/ml_intro_95_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deviation</span> <span class="o">=</span> <span class="n">pred_train_final</span> <span class="o">-</span> <span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">dev_min</span><span class="p">,</span> <span class="n">dev_max</span> <span class="o">=</span> <span class="n">deviation</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">deviation</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">deviation</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">dev_min</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">dev_max</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">bin_width</span> <span class="o">=</span> <span class="p">(</span><span class="n">dev_max</span><span class="o">-</span><span class="n">dev_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">50</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dev_min</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">bin_width</span><span class="p">,</span> <span class="n">dev_min</span> <span class="o">+</span>
                    <span class="mi">50</span><span class="o">*</span><span class="n">bin_width</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">centers</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">bin_width</span><span class="o">*</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\hat{\tilde</span><span class="si">{u}</span><span class="s2">}_x - \tilde</span><span class="si">{u}</span><span class="s2">_</span><span class="si">{x}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;bin count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">/profile_error_hist.svg&quot;</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_96_0.png" src="../../_images/ml_intro_96_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">max_error_map</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">err</span><span class="p">:</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">bins</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pt</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Divide feature space in bins and compute maximum error for each bin.</span>

<span class="sd">    :param x: feature tensor</span>
<span class="sd">    :type x: pt.Tensor</span>
<span class="sd">    :param err: absolute error values</span>
<span class="sd">    :type err: pt.Tensor</span>
<span class="sd">    :param bins: number of bin</span>
<span class="sd">    :type bins: int</span>
<span class="sd">    :return: 2D tensor with maximum errors for each bin; if there are N bins,</span>
<span class="sd">        the result will be a NxN tensor</span>
<span class="sd">    :rtype: pt.Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="n">bins</span>
    <span class="n">bounds_low</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">bounds_up</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">+</span><span class="n">step</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">step</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">max_error</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">bins</span><span class="p">,</span> <span class="n">bins</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">low_i</span><span class="p">,</span> <span class="n">up_i</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">bounds_low</span><span class="p">,</span> <span class="n">bounds_up</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">low_j</span><span class="p">,</span> <span class="n">up_j</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">bounds_low</span><span class="p">,</span> <span class="n">bounds_up</span><span class="p">)):</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">low_i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">low_j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span>
                <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">up_i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">up_j</span><span class="p">),</span> <span class="n">err</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">float64</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">max_error</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">max_error</span>


<span class="n">error_map</span> <span class="o">=</span> <span class="n">max_error_map</span><span class="p">(</span><span class="n">feature_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">train_tensor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">deviation</span><span class="o">.</span><span class="n">abs</span><span class="p">(),</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>
<span class="n">pcol</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">error_map</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">pcol</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$|\hat{\tilde</span><span class="si">{u}</span><span class="s2">}_x - \tilde</span><span class="si">{u}</span><span class="s2">_</span><span class="si">{x}</span><span class="s2">|$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{y}</span><span class="s2">_</span><span class="si">{norm}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$Re_</span><span class="si">{norm}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;profile_error_heatmap.svg&quot;</span><span class="p">),</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ml_intro_97_0.png" src="../../_images/ml_intro_97_0.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/ml_comp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ml_comp_intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Overview</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By thangckt<br/>
  
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>