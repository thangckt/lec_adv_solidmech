{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Why combine Computational and ML?\n",
    "\n",
    "**Simulations produce massive amounts of data.** The most demanding type of simulation is a so-called *direct numerical simulation* (DNS). DNS focuses on resolving all spatial and temporal scales. Finding patterns in such data to create insights or *compact representations* is usually not possible by means of a simple visual inspection but requires automated processing. “Insights” is a relatively broad term. What it usually means is that we want to find a relatively simple explanation or *model* for a phenomenon.\n",
    "\n",
    "Why are simple representations (insights) important? Besides the sheer pleasure of finding such patterns, they may be relevant for optimizing technical applications. On the one hand side, a simple model of a phenomenon can guide us to modify our application such that the phenomenon is avoided. \n",
    "\n",
    "ML is a key technique to discover such simplified models.\n",
    "\n",
    "**Simulations require data and representations thereof** DNS requires some data as input to produce meaningful outputs. As we introduce more physical models in our simulation, the number of closure models and tunable parameters increases. The free parameters must be tuned based on available data sources. Often, a compromise must be found between the complexity of a model and its ability to reflect the data.\n",
    "\n",
    "But also boundary and initial conditions must be known. If these conditions are oversimplified, we can not expect to find good agreement with real flows. If experimental or high-fidelity numerical data is available, we could use that data to create a much more realistic simulation setup and improve the accuracy of our results. However, mapping a data source to the target application is not always straightforward because the data might contain noise or they might be simply too large to deal with them a the run time of the simulation.\n",
    "\n",
    "Again, ML is a suitable tool because it allows creating representations of data that are much more convenient to use in other applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of problems that ML solves\n",
    "\n",
    "Solving problems in computational mechanics typically involves mathematical, physical, and numerical modeling. Not all of these tasks are solved by ML. \n",
    "- Data must be available or it must be possible to generate them. \n",
    "- There is a relatively fixed set of problems that ML solves. \n",
    "\n",
    "It is useful to have a simple mind map to remember these problems or learning categories. \n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Supervised learning deals with two types of tasks: **regression** and **classification**. For both tasks, we need examples of a given input and the expected output. The input values are typically called **features**. The expected output values are termed **labels**. In supervised ML, we create *a mapping from the feature to the label space* based on the examples we have. Then, there are two cases:\n",
    "- If the variable of interest is continuous, the learning task is called *regression*. \n",
    "- In contrast, *classification* deals with the prediction of categorical variables. \n",
    "\n",
    "In the figure below, the plot on the left side depicts a typical regression problem. We have a dataset containing the drag coefficient recorded for different Reynolds numbers. The Reynolds number is the feature and the drag coefficient is the label in this example. A simple ML model could now be trained to make predictions similar to the red dashed line. On the right side, we have a typical classification problem. The dataset consists of two features, the Reynolds number and the angle of attack, and a categorical label with the categories being *laminar* and *turbulent*. A classification model could now learn to predict the correct category for a given input pair of Reynolds number and angle of attack. Internally, the model would learn some representation of the dividing line depicted in red.\n",
    "\n",
    "<img src=\"image/regression_classification.svg\" style=\"width:600px\">\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised learning also solves two types of problems, namely **dimensionality reduction** and **clustering**. No clear distinction between features and labels must be known to apply unsupervised learning algorithms. \n",
    "- Dimensionality reduction techniques aim to find more suitable coordinate systems, which allow to represent the data with *fewer coordinates* than the number of natural coordinates the data are provided. In the example problem depicted below, we could replace the natural coordinates the data are given in, $x_1$ and $x_2$, with a single coordinate $y_1$ if we are willing to accept a small information loss. By storing the data in the reduced coordinate system, every data point would be represented by its $y_1$ value, which corresponds to the orthogonal projection of each point onto $y_1$. Of course, we would lose the information about the distance to the $y_1$ axis, for which we would need a second coordinate $y_2$ orthogonal to $y_1$. However, sometimes it can be even advantageous to lose information, for example, if the deviation normal to $y_1$ was actually some measurement noise. In that case, reducing the dimensionality would also reduces the noise in the data. Also the second type of unsupervised learning, clustering, may be used to remove spurious data. \n",
    "- In clustering, we define a metric to measure the distance between data points in the feature space. Groups of points close to one another form so-called clusters. For the example on the right side in the figure below, we could use the Euclidean distance between points in the plane spanned by $x_1$ and $x_2$. Clustering may be used to find isolated points that do not really belong to any of the other clusters (they form their own cluster with only one point in it). Finding those point might be interesting because they may represent outliers (unwanted data), which we want to remove or avoid.\n",
    "\n",
    "<img src=\"image/unsupervised.svg\" style=\"width:600px=\">\n",
    "\n",
    "### Reinforcement learning   \n",
    "\n",
    "In reinforcement learning, our aim is to *solve control problems*. On the top-level, the reinforcement learning setting consists of two elements, the **agent** and the **environment**. \n",
    "- The agent is basically the controller we want to optimize. \n",
    "- The environment is defined by a state and an intrinsic goal. The intrinsic goal is expressed by the environment as a so-called reward signal. \n",
    "\n",
    "A typical goal in fluid mechanics would be to reduce the drag force acting on a body. From an engineering perspective, we would probably not associate drag reduction as an intrinsic goal of the object the force is acting on, but we will stick to this notion, since it is the standard way to formulate reinforcement learning problems. The agent has a mean of acting on the environment such that the environment's state changes. The agent choses its action based on a set of rules and the current state of the environment. This set of rules is also called *policy*. At the beginning of the learning process, the policy is unlikely to be optimal. However, thanks to the reward signal provided by the environment, the agent has data to improve its policy. Coming back to the flow control problem, the environment's state might be defined by pressure probes taken on the object's surface. The reward signal would be some function based on the currently acting drag force. The function would return high values if the drag force is low and low values if the drag force is high, because we want to maximize the cumulatively received rewards (by definition). The agent's action could be to inject mass somewhere on the object's surface by opening a valve. The policy would be the actual control law that tells us, based on the currently measured pressure value, when to open or close the valves such that the drag forces becomes minimal.\n",
    "\n",
    "<img src=\"image/reinforcement.svg\" style=\"width:600px\">\n",
    "\n",
    "### Other types of learning\n",
    "\n",
    "Of course, it is also possible to combine different types of learning to solve a problem. One example is semi-supervised learning, where a few feature-labels-pairs are provided initially to associate elements in an unlabeled dataset with the most suitable label, e.g, by clustering.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges encountered in ML\n",
    "\n",
    "Of course, ML does not come for free and introduces new challenges. The *learning* in ML is nothing but the *optimization of free parameters based on data*. \n",
    "- This optimization is often high-dimensional and non-linear. \n",
    "- One has to deal with potential flaws in the datasets like spurious or missing values. \n",
    "- Another challenge is the assessment of the ML model's predictive quality. It might be that the models reflect the dataset extremely well while failing to interpolate between the data points. The latter scenario would be called *over-fitting* in ML terminology. \n",
    "\n",
    "Luckily, there are well-established techniques to deal with non-linear optimization and over-fitting. However, a good rule of thumb is that all ML-related problems become much easier to deal with *if there is enough data and the data are of high quality*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to combine Computational and ML\n",
    "\n",
    "In the examples provided in the previous sections, ML modeling interacted in different ways with the CFD simulation. There are typical processing pipelines to categorize this interaction. These categories are sketched below.\n",
    "\n",
    "**ML as post-postprocessing tool**\n",
    "The most common and natural workflow is to use ML as a means for post-processing the simulation data. The simulation generates characteristic flow data like velocity, pressure, or temperature field but also secondary data like forces acting on objects or the heat flux through a wall. ML could then be used to create a regression model, e.g., to predict the heat flux based on the inflow velocity, or to extract patterns from the flow fields, e.g, by finding coherent structures in turbulent flows by means of dimensionality reduction.\n",
    "\n",
    "<img src=\"image/data_workflows_2.svg\" style=\"width:500px\">\n",
    "\n",
    "**Use ML generate input data for simulation**\n",
    "A second typical workflow is to create a representation of an available dataset such that we can use the data/model to improve the simulation. A potential scenario might be that particle image velocimetry (PIV) provides a time series of velocity vectors in one plane of the experiment. Let's say we also want to perform a CFD simulation of the experimental setup to obtain 3D velocity and pressure fields in addition to the PIV data. We could now train a regression model that learns to predict the inlet velocity for the simulation as a function of time based on the PIV data. The model would be very simple to evaluate, perform interpolation tasks for us, and could potentially also remove some of the noise in the experimental data. Realistic initial and boundary conditions are essential to find good agreement with experimental data.\n",
    "\n",
    "<img src=\"image/data_workflows_1.svg\" style=\"width:500px\">\n",
    "\n",
    "**Computational-ML pipeline**\n",
    "Of course, both of the workflows outlined before can be combined and repeated in some kind of Computational-ML pipeline. A typical scenario for such a pipeline would be *scale-up modeling*. Scale-up modeling starts with a limited range of small scales being investigated with dedicated numerical tools. These tool resolve phenomena with great detail and require little modeling. In the next step, the range of investigated scales is extended. Small scales are modeled based on the previously conducted small scale investigations. Then, the range of scales is extended again and this process continues until the full range of scales is reached. In the field of multiphase flow reactors, the pipeline of simulation tools might look as follows:\n",
    "\n",
    "1. single-phase simulations of reactive boundary layers\n",
    "2. particle-resolved multiphase simulations of individual fluid particles with reactive mass transfer\n",
    "3. Euler-Lagrange simulations with the fluid particles being abstracted as point-particles\n",
    "4. Euler-Euler simulations with transport equations for gas-liquid mixtures\n",
    "\n",
    "ML can be used in this pipeline to transport information from one simulation approach to the next.\n",
    "\n",
    "<img src=\"image/data_workflows_3.svg\" style=\"width:600px\">\n",
    "\n",
    "**Computationl-ML Intergration**\n",
    "The last workflow presented here is slightly different from the previous ones. The simulation generates data based on which a ML model is trained. The model is then used in the simulation to generate new data based on which the previous model is further improved. The iterative process is typical for reinforcement learning or direct shape optimization.\n",
    "\n",
    "<img src=\"image/data_workflows_4.svg\" style=\"width:350px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends in ML\n",
    "\n",
    "### Physics-informed neural networks (PINNs)\n",
    "\n",
    "The term physics-informed neural network (PINN) was first coined by [M. Raissi et al. (2017)](https://arxiv.org/abs/1711.10561). However, the basic concept of PINNs was already introduced in [I. E. Lagaris et al. (1997)](https://arxiv.org/abs/physics/9705023) about 20 years earlier. There exists a large body of literature concerned with the approximate solution of ordinary differential equations (ODEs), partial differential equations (PDEs), and systems of ODEs and PDEs in various disciplines. However, with the publication of libraries like Tensorflow in 2014 and PyTorch in 2017, the concept of transforming an initial/boundary value problem into an optimization problem became significantly easier to implement. Some of main improvements of the last decade enabling PINNs are:\n",
    "\n",
    "- powerful automatic differentiation engines (backpropagation)\n",
    "- affordable and fast hardware accelerators (GPUs, TPUs)\n",
    "- ability to train deep neural networks (weight initialization, batch normalization, optimizer)\n",
    "- libraries with high-level abstractions for building and training neural networks.\n",
    "\n",
    "In this course, we do not make use of PINNs . Nonetheless, we introduce the core idea, since PINNs might become more relevant in future applications. Specifically, we do not even train a physics-informed *neural network* but a physics-informed *polynomial* for simplicity. The polynomial is not as flexible as the network, but the general steps to a find a solution are the same.\n",
    "\n",
    "Consider the following initial values problem:\n",
    "\n",
    "$$\n",
    "  \\frac{\\mathrm{d}x}{\\mathrm{d}t} = -kx\\quad \\text{with}\\quad x(t=0)=1\\quad \\text{and}\\quad x\\in\\left[0, 1\\right].\n",
    "$$\n",
    "The problem's solution is \n",
    "\n",
    "$$\n",
    "  x(t) = \\mathrm{exp}\\left(-kx\\right).\n",
    "$$\n",
    "\n",
    "Let's start by plotting this function with Matplotlib. Moreover, we use PyTorch tensors as array data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# increase plot resolution\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mfigure.dpi\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m160\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch as pt\n",
    "\n",
    "# increase plot resolution\n",
    "plt.rcParams[\"figure.dpi\"] = 160\n",
    "\n",
    "# create output directory\n",
    "output = \"output\"\n",
    "!mkdir -p $output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "1. [Introduction ML in CFD](https://thangckt.github.io/lec_ml_cfd/notebooks/ml_cfd_intro.html)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c79f991f86eb27795e10a995cddc62632fb02f5594cc68269302f756ca9db70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
